1. Network setup: 


NFS Clients               Isilon Nodes
-----------               ------------

                 +-------->  Node0  (/prod) 
 Client0  <------+
                 +-------->  Node1  (/prod_status)

 Client1  <--------------->  Node2  (/prod)

 Client2  <--------------->  Node3  (/prod)

 Client3  <--------------->  Node4  (/prod)

 Client4  <--------------->  Node5  (/prod)

 Client5  <--------------->  Node6  (/prod)

 Client6  <--------------->  Node7  (/prod)

 Client7  <--------------->  Node8  (/prod)

 Client8  <--------------->  Node9  (/prod)

 Client9  <--------------->  Node10 (/prod)

 Client10 <--------------->  Node11 (/prod)
  
 Client11 <--------------->  Node12 (/prod)

 Client12 <--------------->  Node13 (/prod)


2. Place all scripts and data files in /prod/tmp/isilon_testing.  This includes:  

   size-buckets.dat
   temerity-benchmark-init
   temerity-benchmark
   status-loop
   status-loop-dist
   status-loop-split
   run-wedge  
   run-wedge-dist
   run-wedge-split
   strip-wedge-data
   strip-wedge-data-dist
   strip-wedge-data-split
   size-histogram
   checksum-copy-threads
   checksum-copy-threads-dist
   checksum-copy-threads-split

   You can use a different testing directory, but if you do you'll need to modify the scripts which have this
   location hardcoded into them using a variable called $tdir nead the top of each script.


2. Initialized 17 sets of independent data files:

  for (( x=0; x<17; x++))
  do
    ./temerity-benchmark-init /prod/tmp/test$x /prod/tmp/test$x /usr/tmp/test$x /prod/tmp/size-buckets.dat 17179869184
    mv benchmark-init.data benchmark-init$x.data 
  done

  For IC used /prod/tmp on the Isilon for the file read/write areas and /usr/tmp for the checksum write area.  Modify
  the "benchmark-init*.data" files to suit your setup if you want to use a different directory.  The test0 areas are 
  used for status operations and the test1-17 are used for checksum reads copy read/write operations.  There are 
  independent areas so that none of the tests are using the same files or directories.  

  Each test uses roughly 16GB worth of files distributed among files of various sizes determined by the
  "size-buckets.dat" file included.  This file was generated by analyzing IC's production data.  

  The "benchmark-init.data" files describe where each test reads and writes and what files it should process.  For
  each set of tests, we'll be changing the locations to read/write so we need 3 sets of these files and one file for
  each independent benchmark test running in parallel.   At this point you should have 17 "benchmark-init*.data" files
  numbered 0-16.  We'll them modify these to create two more sets of 17 files: 

  for (( x=1; x<17; x++))
  do
    cp benchmark-init$x.data benchmark-init-split$x.data 
    cp benchmark-init$x.data benchmark-init-dist$x.data 
  done

  cat benchmark-init0.data | sed -e 's:/prod/:/prod_status/:' > benchmark-split0.data
  cp benchmark-split0.data  benchmark-dist0.data
  
  
3. Run the first set of benchmarks which use only Client0 and Node0 for checksum/copy tests simultaneous with 
performing status on the same NFS mount.  On Client0: 

  cd /prod/tmp/isilon_testing

  for x in 1 2 4 8 16
  do
    ./run-wedge $x
  done
  
  This will generate "wedge-*" directories containing the output from all tests.  It will also echo the status 
  timing test output to STDOUT so you can monitor what is happening.  After the tests are complete you can 
  inspect the contents of the log directories to see the results.  I've also made some scripts which strip out
  the important timing numbers from these logs which you can run like so: 

  for x in 1 2 4 8 16
  do
    ./strip-wedge-data $x
  done
  
  This will create "status-timing-*.data", "checksum-timing-*.data" and "copy-timing-*.data" files suitable 
  for loading into GNU plot or your favorite spreadsheet.


4. Run the second test which uses only the Client0 box, but talks to both Node0 and Node1 via the /prod and /prod_status mounts.  All status traffic is to Node1 and all checksum/copy traffic to Node0.  The tests are run nearly identically to the first set.  On Client0:

  cd /prod/tmp/isilon_testing

  for x in 1 2 4 8 16
  do
    ./run-wedge-split $x
  done
  
  This will generate "wedge-split-*" directories containing the output from all tests.  It will also echo the status 
  timing test output to STDOUT so you can monitor what is happening.  After the tests are complete you can 
  inspect the contents of the log directories to see the results.  I've also made some scripts which strip out
  the important timing numbers from these logs which you can run like so: 

  for x in 1 2 4 8 16
  do
    ./strip-wedge-data-split $x
  done
  
  This will create "status-timing-split-*.data", "checksum-timing-split-*.data" and "copy-timing-split-*.data" files 
  suitable for loading into GNU plot or your favorite spreadsheet.


5. Run the third set of tests which perform all status operations from Client0 to Node0, but distribute the checksum
and copy operations between the Client2-12 machines. Each of these checksum/copy clients is configured to only talk
a single Isilon node and none share the same node.  The tests are run nearly identically to the first set.  On Client0:

  Before runningt the scripts, you'll need to setup paswordless SSH login using pub/private keys from Client0 to 
  the other Client* machines.  The "run-wedge-dist" script will login to these machines to run the distributed 
  parts fo the test.  You'll need to modify this script so it generated the correct machine names for your network.
  At IC these clients where named "zr029" - "zr041" and that's what the script current generates.  

  cd /prod/tmp/isilon_testing

  for x in 1 2 4 8 16
  do
    ./run-wedge-dist $x
  done
  
  This will generate "wedge-dist-*" directories containing the output from all tests.  It will also echo the status 
  timing test output to STDOUT so you can monitor what is happening.  After the tests are complete you can 
  inspect the contents of the log directories to see the results.  I've also made some scripts which strip out
  the important timing numbers from these logs which you can run like so: 

  for x in 1 2 4 8 16
  do
    ./strip-wedge-data-dist $x
  done
  
  This will create "status-timing-dist-*.data", "checksum-timing-dist-*.data" and "copy-timing-dist-*.data" files 
  suitable for loading into GNU plot or your favorite spreadsheet.



That's all.
